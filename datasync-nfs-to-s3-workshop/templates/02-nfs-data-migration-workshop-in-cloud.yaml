# Workshop
# https://github.com/aws-samples/aws-datasync-migration-workshop/tree/master/workshops/nfs-migration/EN

AWSTemplateFormatVersion: '2010-09-09'
Description: AWS DataSync Workshop - NFS Migration - IN-CLOUD region
Metadata:
  License:
    Description: |
      Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.

      Permission is hereby granted, free of charge, to any person obtaining a copy of this
      software and associated documentation files (the "Software"), to deal in the Software
      without restriction, including without limitation the rights to use, copy, modify,
      merge, publish, distribute, sublicense, and/or sell copies of the Software, and to
      permit persons to whom the Software is furnished to do so.

      THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,
      INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A
      PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
      HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
      OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
      SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

Resources:

  # We use the GUID from the ARN of the stack ID to generate
  # a unique bucket name
  DemoS3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      PublicAccessBlockConfiguration:
        BlockPublicAcls: True
        BlockPublicPolicy: True
        IgnorePublicAcls: True
        RestrictPublicBuckets: True
      BucketName: !Join
      - "-"
      - - "data-migration-workshop"
        - !Select
          - 2
          - !Split
            - "/"
            - !Ref "AWS::StackId"

      Tags:
        - Key: ResourceName
          Value: DemoS3Bucket
        - Key: CostCenter
          Value: Datasync Migration Workshop

  DemoCustomResourceObject:
    Type: "Custom::S3ObjectsDemo"
    Properties:
      ServiceToken: !GetAtt S3ObjectsFunctionHandler.Arn
      SourceBucket: !Ref DemoS3Bucket
      SourcePrefix: ""
      Bucket: !Ref DemoS3Bucket

  # Give the role a friendly name as the workshop user will need to
  # reference it when creating DataSync tasks.
  DatasyncS3BucketIamRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - datasync.amazonaws.com
        Version: '2012-10-17'
      Tags:
        - Key: Name
          Value: migration-workshop-demo-iamrole
        - Key: ResourceName
          Value: DatasyncS3BucketIamRole
        - Key: CostCenter
          Value: Datasync Migration Workshop

  DatasyncS3BucketInlinePolicy:
    Type: AWS::IAM::Policy
    DependsOn: DemoS3Bucket
    Properties:
      PolicyDocument:
        Statement:
          - Effect: Allow
            Resource: !GetAtt DemoS3Bucket.Arn
            Action:
              - s3:GetBucketLocation
              - s3:ListBucket
              - s3:ListBucketMultipartUploads
              - s3:HeadBucket
          - Effect: Allow
            Resource: !Join [ "/", [ !GetAtt DemoS3Bucket.Arn, "*" ] ]
            Action:
              - s3:AbortMultipartUpload
              - s3:DeleteObject
              - s3:GetObject
              - s3:ListMultipartUploadParts
              - s3:PutObject
              - s3:GetObjectTagging
              - s3:PutObjectTagging
        Version: '2012-10-17'
      PolicyName: s3-readwrite-access-inline-policy
      Roles:
        - !Ref 'DatasyncS3BucketIamRole'

  # Lambda execution role for custom object handler.
  LambdaS3ExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      Path: /
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          -
            Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        -
          PolicyName: s3-read-write-access
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              -
                Sid: AllowLogging
                Effect: Allow
                Action:
                  - "logs:CreateLogGroup"
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                Resource: "*"
              -
                Sid: s3readaccess
                Effect: Allow
                Action:
                  - "s3:ListBucket"
                  - "s3:GetObject"
                Resource:
                  - !Sub "arn:aws:s3:::${DemoS3Bucket}"
                  - !Sub "arn:aws:s3:::${DemoS3Bucket}/*"
              -
                Sid: s3readwriteaccess
                Effect: Allow
                Action:
                  - "s3:ListBucket"
                  - "s3:GetObject"
                  - "s3:PutObject"
                  - "s3:PutObjectAcl"
                  - "s3:PutObjectVersionAcl"
                  - "s3:DeleteObject"
                  - "s3:DeleteObjectVersion"
                  - "s3:CopyObject"
                Resource:
                  - !Sub "arn:aws:s3:::${DemoS3Bucket}"
                  - !Sub "arn:aws:s3:::${DemoS3Bucket}/*"
      Tags:
        - Key: Name
          Value: migration-workshop-demo-lambda-execution-iamrole
        - Key: ResourceName
          Value: LambdaS3ExecutionRole

  # Lambda function for deleting objects.
  S3ObjectsFunctionHandler:
    Type: AWS::Lambda::Function
    Properties:
      Description: Delete all objects in s3 bucket.
      Handler: index.handler
      Runtime: python3.10
      Role: !GetAtt LambdaS3ExecutionRole.Arn
      Timeout: 180
      Code:
        ZipFile: |
          import os 
          import json
          import cfnresponse
          import boto3
          import logging

          from botocore.exceptions import ClientError
          client = boto3.client('s3')
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          def handler(event, context):
            logger.info("Received event: %s" % json.dumps(event))
            source_bucket = event['ResourceProperties']['SourceBucket']
            source_prefix = event['ResourceProperties'].get('SourcePrefix') or ''
            bucket = event['ResourceProperties']['Bucket']
            prefix = event['ResourceProperties'].get('Prefix') or ''

            result = cfnresponse.SUCCESS

            try:
              if event['RequestType'] == 'Delete':
                result = delete_objects(bucket)

            except ClientError as e:
              logger.error('Error: %s', e)
              result = cfnresponse.FAILED

            cfnresponse.send(event, context, result, {})

          # 
          def delete_objects(bucket):
            try:
              # Create an S3 client
              client = boto3.client('s3')

              # List all objects in the bucket
              print('Retrieve objects')
              objects = client.list_objects_v2(Bucket=bucket)
              print(objects)

              # Get the number of objects
              num_objects = len(objects.get('Contents', []))
              print('Found', num_objects, 'objects in bucket:', bucket)

              # Delete each object
              for obj in objects.get('Contents', []):
                  key = obj['Key']
                  try:
                      client.delete_object(Bucket=bucket, Key=key)
                      print(f'Deleted object: {key}')
                  except ClientError as e:
                      print(f'Error deleting object {key}: {e}')

              print('All objects deleted from bucket:', bucket)
              return cfnresponse.SUCCESS

            except:
              return cfnresponse.FAILED

      Tags:
        - Key: ResourceName
          Value: S3ObjectsFunctionHandler
        - Key: CostCenter
          Value: Datasync Migration Workshop

Outputs:
  bucketName:
    Description: S3 Bucket Name
    Value: !Ref 'DemoS3Bucket'

  iamRoleForDataSync:
    Description: IAM Role for DataSync to managed S3 Bucket
    Value: !Ref 'DatasyncS3BucketIamRole'
